{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar y combinar datasets\n",
    "\n",
    "En este bloque, se cargan los datasets desde archivos CSV, se verifica que contengan las columnas necesarias, y se realiza un diagnóstico inicial de los datos. Además, se lleva a cabo una **recodificación de los valores de sentimiento** para unificar el esquema de etiquetas antes de procesar los datos con el modelo.\n",
    "\n",
    "### Recodificación de valores de sentimiento\n",
    "Anteriormente, las etiquetas de sentimiento tenían los siguientes valores:\n",
    "- **-1**: Sentimiento negativo hacia el candidato.\n",
    "- **0**: Neutralidad hacia el candidato o ausencia de mención.\n",
    "- **1**: Sentimiento positivo hacia el candidato.\n",
    "\n",
    "En el nuevo esquema de etiquetas, los valores se recodifican para adaptarse al modelo de clasificación, quedando de la siguiente manera:\n",
    "- **0**: Sentimiento negativo hacia el candidato.\n",
    "- **1**: Neutralidad hacia el candidato o ausencia de mención.\n",
    "- **2**: Sentimiento positivo hacia el candidato.\n",
    "\n",
    "Esta recodificación es fundamental para que el modelo pueda procesar y clasificar correctamente las etiquetas de manera uniforme.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Definir función para transformar etiquetas de sentimiento\n",
    "def sentiment_to_label(x):\n",
    "    if pd.isna(x):\n",
    "        return 1\n",
    "    elif x == -1:\n",
    "        return 0\n",
    "    elif x == 0:\n",
    "        return 1\n",
    "    elif x == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        print(f\"Valor de sentimiento inesperado: {x}\")\n",
    "        return 1\n",
    "\n",
    "# Rutas de los archivos CSV\n",
    "debates_csv = \"datasets/debates.csv\"\n",
    "election_day_csv = \"datasets/election_day.csv\"\n",
    "\n",
    "# Verificar existencia de los archivos\n",
    "if not os.path.isfile(debates_csv) or not os.path.isfile(election_day_csv):\n",
    "    raise FileNotFoundError(\"Alguno de los archivos CSV no fue encontrado.\")\n",
    "\n",
    "# Cargar los datasets\n",
    "debates_df = pd.read_csv(debates_csv)\n",
    "election_day_df = pd.read_csv(election_day_csv)\n",
    "\n",
    "# Verificar las columnas requeridas\n",
    "required_columns = [\"comentario_editado\", \"Xóchitl\", \"Claudia\", \"Maynez\", \"Ninguno\"]\n",
    "for df, name in zip([debates_df, election_day_df], [\"debates.csv\", \"election_day.csv\"]):\n",
    "    missing_cols = set(required_columns) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"El dataset {name} le falta(n) las columnas: {missing_cols}\")\n",
    "\n",
    "# Seleccionar columnas relevantes\n",
    "debates_df = debates_df[required_columns]\n",
    "election_day_df = election_day_df[required_columns]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnóstico y limpieza de datos\n",
    "\n",
    "Este bloque analiza los tipos de datos, los valores únicos y detecta problemas como valores NaN o inesperados. También limpia los datos, eliminando filas con valores inválidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosticar tipos y valores únicos\n",
    "def diagnosticar_tipos_y_valores(df, dataset_name, columnas):\n",
    "    for col in columnas:\n",
    "        unique_values = df[col].unique()\n",
    "        data_type = df[col].dtype\n",
    "        print(f\"Dataset '{dataset_name}', Columna '{col}':\")\n",
    "        print(f\"  Tipo de dato: {data_type}\")\n",
    "        print(f\"  Valores únicos: {unique_values}\\n\")\n",
    "\n",
    "diagnosticar_tipos_y_valores(debates_df, \"debates.csv\", required_columns[1:])\n",
    "diagnosticar_tipos_y_valores(election_day_df, \"election_day.csv\", required_columns[1:])\n",
    "\n",
    "# Limpieza de columnas sentimentales\n",
    "for col in required_columns[1:]:\n",
    "    for df in [debates_df, election_day_df]:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df.dropna(subset=[col], inplace=True)\n",
    "        df = df[df[col].isin([-1, 0, 1])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de etiquetas y combinación final\n",
    "\n",
    "Convierte las etiquetas de sentimiento a un formato numérico para el modelo, limpia los comentarios y combina ambos datasets en uno solo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar transformación de etiquetas\n",
    "for df in [debates_df, election_day_df]:\n",
    "    for col in required_columns[1:]:\n",
    "        df[col] = df[col].apply(sentiment_to_label)\n",
    "\n",
    "# Combinar datasets\n",
    "combined_df = pd.concat([debates_df, election_day_df], ignore_index=True)\n",
    "\n",
    "# Limpiar comentarios no válidos\n",
    "combined_df = combined_df[combined_df['comentario_editado'].apply(lambda x: isinstance(x, str))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División de datos y preparación para Huggingface\n",
    "\n",
    "En este bloque, se divide el dataset combinado en dos conjuntos: 80% para entrenamiento y 20% para validación, utilizando una división aleatoria con `train_test_split`. Posteriormente, los datos se convierten en datasets compatibles con Huggingface.\n",
    "\n",
    "Para que los datasets sean compatibles con los modelos de Huggingface, es necesario incluir las siguientes características generadas por el tokenizador:\n",
    "\n",
    "- **input_ids**: Secuencias de identificadores únicos que representan cada palabra o subpalabra del texto según el vocabulario del modelo preentrenado. Estas son las entradas principales al modelo.\n",
    "- **attention_mask**: Una máscara binaria que indica qué tokens son válidos (1) y cuáles son relleno (`padding`) (0). Esto permite al modelo ignorar los tokens de relleno durante el entrenamiento y la inferencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "# Dividir datos\n",
    "train_df, valid_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir a datasets Huggingface\n",
    "train_dataset_hf = Dataset.from_pandas(train_df)\n",
    "valid_dataset_hf = Dataset.from_pandas(valid_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización\n",
    "\n",
    "En este bloque, se utiliza un tokenizador basado en BETO (BERT en español) para procesar los textos del dataset y convertirlos en una representación numérica adecuada para el modelo.\n",
    "\n",
    "El tokenizador realiza las siguientes funciones principales:\n",
    "- **Divide el texto en subpalabras** según el vocabulario del modelo BETO. Esto permite manejar palabras desconocidas descomponiéndolas en unidades más pequeñas.\n",
    "- **Asigna un identificador único (`input_ids`)** a cada subpalabra basada en el vocabulario del modelo preentrenado.\n",
    "- **Genera una máscara de atención (`attention_mask`)**, que indica qué tokens son válidos (1) y cuáles son tokens de relleno (0), para que el modelo los ignore.\n",
    "\n",
    "Los principales argumentos utilizados en la función de tokenización son:\n",
    "- `text`: El texto que será tokenizado.\n",
    "- `truncation=True`: Indica que los textos largos se truncarán al alcanzar la longitud máxima especificada.\n",
    "- `padding=\"max_length\"`: Asegura que todos los textos tengan la misma longitud añadiendo tokens de relleno si es necesario.\n",
    "- `max_length=128`: Establece la longitud máxima de los textos procesados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"path_to_your_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        text=examples[\"comentario_editado\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# Tokenizar datasets\n",
    "train_dataset_hf = train_dataset_hf.map(tokenize_function, batched=True)\n",
    "valid_dataset_hf = valid_dataset_hf.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del modelo\n",
    "\n",
    "Este bloque define un modelo basado en BETO para la clasificación multi-etiqueta. El modelo está diseñado para predecir el sentimiento hacia múltiples candidatos en un comentario. La arquitectura se compone de dos componentes principales:\n",
    "\n",
    "1. **BETO (BERT en español)**:\n",
    "   - BETO es un modelo preentrenado que transforma las entradas textuales en representaciones vectoriales de alta dimensión.\n",
    "   - BETO recibe como entrada las representaciones tokenizadas (`input_ids` y `attention_mask`).\n",
    "   - Genera dos tipos de salidas principales:\n",
    "     - `last_hidden_state`: Representación contextual de cada token del texto.\n",
    "     - `pooler_output` (o representación del token `[CLS]`): Un vector de 768 dimensiones que encapsula el significado del texto completo.\n",
    "\n",
    "2. **Capa de perceptrón simple**:\n",
    "   - Toma como entrada el vector de 768 dimensiones de BETO (representación del token `[CLS]`).\n",
    "   - Genera una salida de tamaño `num_labels_per_candidate * num_candidates`, donde:\n",
    "     - `num_labels_per_candidate` es la cantidad de categorías por candidato (por ejemplo, 3: negativo, neutral, positivo).\n",
    "     - `num_candidates` es el número total de candidatos.\n",
    "   - Utiliza una capa densa (`Linear`) para calcular las probabilidades para cada etiqueta de cada candidato.\n",
    "\n",
    "### Flujo del modelo:\n",
    "1. BETO toma el texto tokenizado y genera una representación densa de tamaño fijo.\n",
    "2. El perceptrón simple proyecta esta representación en un espacio donde las etiquetas de los candidatos son predichas.\n",
    "3. Las predicciones son ajustadas y evaluadas mediante la función de pérdida `CrossEntropyLoss`, aplicada individualmente a cada candidato.\n",
    "\n",
    "El modelo implementa la clasificación multi-etiqueta al reorganizar las salidas de la capa lineal en un formato adecuado para procesar cada candidato de manera independiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "class BetoMultiOutput(nn.Module):\n",
    "    def __init__(self, model_name, num_labels_per_candidate=3, num_candidates=4):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, num_labels_per_candidate * num_candidates)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]\n",
    "        logits = self.classifier(pooled_output).view(-1, 4, 3)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            losses = [loss_fct(logits[:, i, :], labels[:, i]) for i in range(4)]\n",
    "            loss = torch.stack(losses).mean()\n",
    "        return {\"loss\": loss, \"logits\": logits}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Este bloque configura el `Trainer` de Huggingface para entrenar el modelo con los datasets preparados. El `Trainer` es una herramienta poderosa que facilita el entrenamiento y evaluación de modelos. \n",
    "\n",
    "### Desglose de argumentos del `Trainer`:\n",
    "\n",
    "1. **Modelo (`model`)**:\n",
    "   - El modelo que se va a entrenar. En este caso, es una instancia de la clase `BetoMultiOutput`.\n",
    "\n",
    "2. **Args (`args`)**:\n",
    "   - Contiene los argumentos de configuración del entrenamiento definidos en `TrainingArguments`.\n",
    "\n",
    "### Desglose de `TrainingArguments`:\n",
    "- **output_dir**: Directorio donde se guardarán los checkpoints y el modelo entrenado.\n",
    "- **eval_strategy**: Frecuencia de evaluación. Valores comunes:\n",
    "  - `\"epoch\"`: Evalúa después de cada época.\n",
    "  - `\"steps\"`: Evalúa después de un número específico de pasos.\n",
    "- **save_strategy**: Frecuencia con la que se guardan los checkpoints. Similar a `eval_strategy`.\n",
    "- **num_train_epochs**: Número de épocas (iteraciones completas sobre los datos de entrenamiento).\n",
    "- **per_device_train_batch_size**: Tamaño del batch (cantidad de ejemplos procesados simultáneamente) para entrenamiento.\n",
    "- **per_device_eval_batch_size**: Tamaño del batch para evaluación.\n",
    "- **logging_steps**: Número de pasos de entrenamiento entre registros en el log.\n",
    "- **load_best_model_at_end**: Si es `True`, carga el modelo con el mejor desempeño (según una métrica) al final del entrenamiento.\n",
    "- **metric_for_best_model**: Métrica utilizada para determinar el mejor modelo durante el entrenamiento.\n",
    "- **logging_dir**: Directorio donde se guardan los logs del entrenamiento.\n",
    "\n",
    "3. **Datasets (`train_dataset` y `eval_dataset`)**:\n",
    "   - `train_dataset`: Dataset de entrenamiento.\n",
    "   - `eval_dataset`: Dataset de validación utilizado para evaluar el modelo.\n",
    "\n",
    "4. **Tokenizer (`tokenizer`)**:\n",
    "   - Tokenizador utilizado para preprocesar los datos antes de ingresarlos al modelo.\n",
    "\n",
    "5. **Métricas (`compute_metrics`)**:\n",
    "   - Función personalizada para calcular métricas de evaluación como la exactitud, matriz de confusión, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./beto-multi\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=BetoMultiOutput(model_name=model_path),\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_hf,\n",
    "    eval_dataset=valid_dataset_hf,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación y guardado del modelo\n",
    "\n",
    "Evalúa el modelo en el conjunto de validación y guarda el modelo ajustado junto con el tokenizador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Resultados de evaluación:\", eval_results)\n",
    "\n",
    "# Guardar modelo y tokenizador\n",
    "save_path = \"./mi_beto_finetuned\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
